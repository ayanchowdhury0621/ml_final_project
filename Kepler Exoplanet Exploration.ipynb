{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import RobustScaler \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN, LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Search for Exoplanets\n",
    "### Anika Das, Ayan Chowdhury, Gary Shetye, Grace Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in dataframes\n",
    "planets_train = pd.read_csv('exoTrain.csv')\n",
    "planets_test = pd.read_csv('exoTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the labels of (1, 2) to (0, 1)\n",
    "def convert_labels_to_binary(labels, label_one, label_two):\n",
    "    \n",
    "    binary_labels = []\n",
    "    \n",
    "    # for each label \n",
    "    for label in labels:\n",
    "        # append 0 if the label is label_one (1)\n",
    "        if (label == label_one):\n",
    "            binary_labels.append(0)\n",
    "        # append 1 if the label is label_two (2)\n",
    "        elif (label == label_two):\n",
    "            binary_labels.append(1)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "            \n",
    "    return binary_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels of train and test data to have 0, 1's\n",
    "planets_train_new_labels = convert_labels_to_binary(planets_train[\"LABEL\"].tolist(), 1, 2)\n",
    "planets_test_new_labels = convert_labels_to_binary(planets_test[\"LABEL\"].tolist(), 1, 2)\n",
    "\n",
    "# change dataframe outcome column to new labels \n",
    "planets_train[\"LABEL\"] = planets_train_new_labels\n",
    "planets_test[\"LABEL\"] = planets_test_new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x_and_outcome(df, outcome_col):\n",
    "    \n",
    "    # get df of all features (no outcome column)\n",
    "    features = df.loc[:, df.columns != outcome_col]\n",
    "    # get Series object of outcome column\n",
    "    outcome = df[outcome_col]\n",
    "    \n",
    "    return features, outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and testing dataframes into their features and labels\n",
    "planets_features_train = split_x_and_outcome(planets_train, \"LABEL\")[0]\n",
    "planets_labels_train = split_x_and_outcome(planets_train, \"LABEL\")[1]\n",
    "\n",
    "planets_features_test = split_x_and_outcome(planets_test, \"LABEL\")[0]\n",
    "planets_labels_test = split_x_and_outcome(planets_test, \"LABEL\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust scaler\n",
    "def scaler(df):\n",
    "    scaler = RobustScaler()\n",
    "    # takes a dataframe, transposes it, scales each column (each star's flux values)\n",
    "    # based on median and IQR, and transposes it back\n",
    "    scaled = pd.DataFrame(scaler.fit_transform(df.T).T,columns=df.columns)\n",
    "    return scaled\n",
    "\n",
    "# Synthetic minority sampling technique (SMOTE) to balance 0/1 classes\n",
    "def minority_sampling(df, labels, neighbors):\n",
    "    # initialize SMOTE object\n",
    "    oversample = SMOTE(k_neighbors=neighbors)\n",
    "    # fit SMOTE to create df with synthetic samples\n",
    "    df, labels = oversample.fit_resample(df, labels)\n",
    "    return df, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Robust scaler to training and testing features\n",
    "features_scaled = scaler(planets_features_train)\n",
    "features_test_scaled = scaler(planets_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create equal number of 0/1 samples \n",
    "added_rows = minority_sampling(features_scaled, planets_labels_train, 3) # shape (10100, 3197)\n",
    "added_rows_test = minority_sampling(features_test_scaled, planets_labels_test, 3) # shape (1130, 3197)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to calculate euclidean distance\n",
    "def euclidean_dist(r1, r2):\n",
    "    r1_np = np.array(r1)\n",
    "    r2_np = np.array(r2)\n",
    "    \n",
    "    dist = np.linalg.norm(r1_np - r2_np)\n",
    "    return dist\n",
    "\n",
    "# helper function to calculate manhattan distance\n",
    "def manhattan(a, b):\n",
    "    return sum(abs(val1-val2) for val1, val2 in zip(a,b))\n",
    "\n",
    "\n",
    "# helper function returning most common value in a list\n",
    "def most_common(speaker_preds):\n",
    "    # returns most common value in a list\n",
    "    data = Counter(speaker_preds)\n",
    "    return data.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_metrics(y_actuals, y_pred):\n",
    "    \n",
    "    '''\n",
    "    Given a series of actual labels (y), and a series of predicted outcomes (ypred), \n",
    "    returns the model accuracy, sensitivity, specificity, precision, and f1-score. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # initializing counts + metrics variables \n",
    "    false_pos = 0 \n",
    "    false_neg = 0\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    \n",
    "    recall = 0\n",
    "    precision = 0\n",
    "    f1_score = 0\n",
    "    accuracy = 0\n",
    "    specificity = 0\n",
    "    \n",
    "    # for each prediction\n",
    "    for i in range(len(y_pred)):\n",
    "        # if the prediction was obama\n",
    "        if (y_pred[i] == 0):\n",
    "            # sentence actually obama's\n",
    "            if (y_actuals[i] == 0):\n",
    "                # true obama found\n",
    "                true_neg += 1\n",
    "            else:\n",
    "                # sentence actually trump's\n",
    "                false_neg += 1\n",
    "        # if the prediction was trump\n",
    "        else:\n",
    "            # if the sentence actually trump's\n",
    "            if (y_actuals[i] == 1):\n",
    "                # true positive found\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                # sentence actually obama's\n",
    "                false_pos += 1\n",
    "    \n",
    "    # no true positives --> f1 must be undefined\n",
    "    if (true_pos == 0):\n",
    "        f1_score = None\n",
    "        # no false positives means precision is undefined\n",
    "        if (false_pos == 0):\n",
    "            precision = None\n",
    "        else:\n",
    "            # precision is 0 if there are some false positives\n",
    "            precision = 0\n",
    "        # no false negatives means that recall is undefined\n",
    "        if (false_neg == 0):\n",
    "            recall = None\n",
    "        else:\n",
    "            # recall is 0 if there are some false negatives\n",
    "            recall = 0\n",
    "    else:\n",
    "        # calculating precision, recall, f1 outside of edge cases\n",
    "        precision = round((true_pos) / (true_pos + false_pos), 7)\n",
    "        recall = round((true_pos) / (true_pos + false_neg), 7)\n",
    "        f1_score = round(2*((recall * precision) / (recall + precision)), 7)\n",
    "       \n",
    "    # calculating accuracy, specificity\n",
    "    try:\n",
    "        specificity = round((true_neg) / (true_neg + false_pos), 3)\n",
    "        accuracy = round((true_pos + true_neg) / len(y_pred), 3)\n",
    "    except ZeroDivisionError:\n",
    "        specificity = 0\n",
    "        accuracy = 0\n",
    "  \n",
    "    return accuracy, precision, specificity, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(train_data, train_labels, val_sentence, num_neighbors, metric):\n",
    "    \n",
    "    preds = []\n",
    "    distances = {}\n",
    "        \n",
    "    # for every training vector\n",
    "    for i in range(len(train_data) - 1):\n",
    "        # calculate euclidean/manhattan distance between training and validation vector. add this distance + train label as \n",
    "        # key-value pair\n",
    "        distances[metric(val_sentence, train_data[i])] = train_labels[i]\n",
    "    \n",
    "    # sort dictionary using OrderedDict\n",
    "    distances = OrderedDict(sorted(distances.items()))\n",
    "    distances = list(distances.items())\n",
    "    # get list of nearest neighbors (train vectors with smallest distances)\n",
    "    neighbors = distances[:num_neighbors]\n",
    "\n",
    "    # get neighbor vector's labels\n",
    "    for pred in neighbors:\n",
    "        preds.append(pred[1])\n",
    "    \n",
    "    # return most common label\n",
    "    return most_common(preds)\n",
    "\n",
    "\n",
    "# k fold validation\n",
    "def split_df_into_folds(df, k):\n",
    "    \n",
    "    # randomly shuffle the dataframe\n",
    "    # shuffle_df = df.sample(frac=1)\n",
    "    # split the dataframe into k folds, store each in an np array\n",
    "    shuffle_df = np.array_split(df, k)\n",
    "    return shuffle_df\n",
    "\n",
    "# accuracy metric\n",
    "def avg_accuracy(y_preds, actual):\n",
    "    \n",
    "    total_accuracy = 0\n",
    "    idx = 0\n",
    "    \n",
    "    true_neg = 0\n",
    "    true_pos = 0\n",
    "    \n",
    "    for y in y_preds:\n",
    "        \n",
    "        if (y == 0):\n",
    "            # trump prediction is correct\n",
    "            if (actual[idx] == 0):\n",
    "                # true negative found\n",
    "                true_neg += 1\n",
    "        if (y == 1):\n",
    "            # obama prediction is correct\n",
    "            if (actual[idx] == 1):\n",
    "                # true negative found\n",
    "                true_pos += 1\n",
    "        \n",
    "        idx += 1\n",
    "    \n",
    "    # return true predictions over all predictions\n",
    "    return (true_pos + true_neg) / len(actual)\n",
    "\n",
    "\n",
    "def k_fold_validation(df, num_folds, penalty, solver, k, outcome_col, label_one, label_two, classifier, metric):\n",
    "    \n",
    "    algorithm = 0\n",
    "    if (classifier == get_neighbors):\n",
    "        algorithm = 1\n",
    "        \n",
    "    # get np array of dataframes for number of folds\n",
    "    dfs = split_df_into_folds(df, num_folds)\n",
    "    \n",
    "    fold_accuracies = {}\n",
    "    evals = []\n",
    "    \n",
    "    for i in range(len(dfs)):\n",
    "        # create new DF\n",
    "        train_dfs = pd.DataFrame()\n",
    "        \n",
    "        # the current df is the validation df, split it into x and outcome\n",
    "        val_df_x = split_x_and_outcome(dfs[i], outcome_col)[0]\n",
    "        val_df_y = convert_labels_to_binary(split_x_and_outcome(dfs[i], outcome_col)[1].squeeze(), label_one, label_two)\n",
    "        # all other df's are for training\n",
    "        others = dfs[:i] + dfs[i+1:]\n",
    "        for dataframe in others:\n",
    "            # make one big DF for training from other df's \n",
    "            train_dfs = pd.concat([train_dfs, dataframe])\n",
    "    \n",
    "        # split training df's into x and outcome\n",
    "        train_dfs_x = split_x_and_outcome(train_dfs, outcome_col)[0]\n",
    "        train_dfs_y = convert_labels_to_binary(split_x_and_outcome(train_dfs, outcome_col)[1].squeeze(), label_one, label_two)\n",
    "        \n",
    "        training_vectors = train_dfs_x.values.tolist()\n",
    "        \n",
    "        if (algorithm == 1):\n",
    "            y_preds = []\n",
    "\n",
    "            # for each sentence, \n",
    "            for i in range(len(val_df_x)):\n",
    "                y_preds.append(classifier(training_vectors, train_dfs_y, val_df_x.iloc[i], k, metric))\n",
    "        else:\n",
    "            y_preds = []\n",
    "            y_preds = classifier(training_vectors, train_dfs_y, val_df_x, k, penalty, solver)\n",
    "        \n",
    "        # add accuracy to dictionary\n",
    "        fold_accuracies[i] = avg_accuracy(y_preds, val_df_y)\n",
    "        evals.append(model_metrics(val_df_y, y_preds))\n",
    "    \n",
    "    # return dictionary of accuracies for each fold's training result\n",
    "    return fold_accuracies, evals\n",
    "\n",
    "\n",
    "def split_x_and_outcome(df, outcome_col):\n",
    "    \n",
    "    # get df of all features (no outcome column)\n",
    "    features = df.loc[:, df.columns != outcome_col]\n",
    "    # get Series object of outcome column\n",
    "    outcome = df[outcome_col]\n",
    "    \n",
    "    return features, outcome\n",
    "\n",
    "\n",
    "def get_avg_accuracy_per_fold(df, num_folds, penalty, solver, k, outcome_col, label_one, label_two, classifier, metric):\n",
    "    \n",
    "    # get the dictionary of accuracies from each fold's training result\n",
    "    accuracies_per_fold = k_fold_validation(df, num_folds, penalty, solver, k, outcome_col, label_one, label_two, classifier, metric)[0]\n",
    "    total_accuracy = 0\n",
    "    \n",
    "    # for each accuracy, add it to total_accuracy\n",
    "    for accuracy in list(accuracies_per_fold.values()):\n",
    "        total_accuracy += accuracy\n",
    "    \n",
    "    # return average accuracy (total accuracy / number of accuracies calculated)\n",
    "    return total_accuracy / len(list(accuracies_per_fold.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_hypertune(df, num_folds, ks: list, outcome_col, label_one, label_two, classifier, metrics: list):\n",
    "    \n",
    "    best_score = 0\n",
    "    best_vals = []\n",
    "    # for each k value\n",
    "    for k in ks:\n",
    "        # for each metric\n",
    "        for metric in metrics:\n",
    "            # get average accuracy per k-fold\n",
    "            curr_accuracy = get_avg_accuracy_per_fold(df[0].sample(frac=0.1), num_folds, k, 0, 1, outcome_col, label_one, label_two, classifier, metric)\n",
    "            print(\"metric: \", metric)\n",
    "            print(\"k: \", k)\n",
    "            print(\"current_Acc: \", curr_accuracy)\n",
    "            # if current accuracy with parameters is better than then current best accuracy\n",
    "            # update the best hyperparameters\n",
    "            if (curr_accuracy > best_score):\n",
    "                best_score = curr_accuracy\n",
    "                best_vals = [k, metric]\n",
    "    \n",
    "    return best_score, best_vals   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric:  <function euclidean_dist at 0x7fcf19fff7a0>\n",
      "k:  5\n",
      "current_Acc:  0.698019801980198\n",
      "metric:  <function manhattan at 0x7fcf19fff050>\n",
      "k:  5\n",
      "current_Acc:  0.7722772277227723\n",
      "metric:  <function euclidean_dist at 0x7fcf19fff7a0>\n",
      "k:  7\n",
      "current_Acc:  0.7722772277227723\n",
      "metric:  <function manhattan at 0x7fcf19fff050>\n",
      "k:  7\n",
      "current_Acc:  0.698019801980198\n",
      "0.7722772277227723\n",
      "[5, <function manhattan at 0x7fcf19fff050>]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters for KNN: nearest-neighbors, distance measurement for instances\n",
    "ks = [5, 7]\n",
    "metrics = [euclidean_dist, manhattan]\n",
    "\n",
    "# add labels back to dataframe\n",
    "added_rows[0][\"LABEL\"] = added_rows[1]\n",
    "\n",
    "# get the best score and values\n",
    "best_score, best_vals = knn_hypertune(added_rows, 5, ks, \"LABEL\", 0, 1, get_neighbors, metrics)\n",
    "print(best_score)\n",
    "print(best_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_test_with_best_parameters(test_features, test_labels, best_k, best_metric):\n",
    "    \n",
    "    y_preds = []\n",
    "    # get the test instances as a list of lists\n",
    "    test_features_list = test_features.values.tolist()\n",
    "    # for each start instance\n",
    "    for i in range(len(test_features)):\n",
    "        rest_features = []\n",
    "        # get the particular instance we are at\n",
    "        test_row = test_features_list[i]\n",
    "        # make a list of lists of every other instance\n",
    "        rest_features = test_features_list[:i] + test_features_list[i:]\n",
    "        # call get neighbors and append predicted class to y_pred\n",
    "        y_preds.append(get_neighbors(rest_features, test_labels.tolist(), test_row, best_k, best_metric))\n",
    "    \n",
    "    # calculate model metrics of y_preds and test labels\n",
    "    metrics = model_metrics(test_labels, y_preds)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.983, 0.9674658, 0.966, 1.0, 0.9834639)\n"
     ]
    }
   ],
   "source": [
    "# calculating testing metrics for KNN\n",
    "metrics = knn_test_with_best_parameters(added_rows_test[0], added_rows_test[1], 7, euclidean_dist)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(training_features, training_labels, test_features, k, pen, sol):\n",
    "    \n",
    "    # instantiate the model (using the default parameters)\n",
    "    logreg = LogisticRegression(penalty=pen, solver=sol, random_state=16)\n",
    "\n",
    "    # fit the model with data\n",
    "    logreg.fit(training_features, training_labels)\n",
    "    \n",
    "    # call predict function\n",
    "    y_pred = logreg.predict(test_features)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_hypertune(df, num_folds, penalties: list, solvers: list, ks: [], outcome_col, label_one, label_two, classifier, metrics: list):\n",
    "    \n",
    "    best_score = 0\n",
    "    best_vals = []\n",
    "    # for each penalty \n",
    "    for penalty in penalties:\n",
    "        # for each solver\n",
    "        for solver in solvers:\n",
    "            # get average accuracy per k-fold\n",
    "            curr_accuracy = get_avg_accuracy_per_fold(df[0].sample(frac=0.1), num_folds, penalty, solver, 0, outcome_col, label_one, label_two, classifier, [])\n",
    "            # if current accuracy with parameters is better than then current best accuracy\n",
    "            # update the best hyperparameters\n",
    "            if (curr_accuracy > best_score):\n",
    "                best_score = curr_accuracy\n",
    "                best_vals = [solver, penalty]\n",
    "    \n",
    "    return best_score, best_vals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9475247524752476\n",
      "['liblinear', 'l1']\n"
     ]
    }
   ],
   "source": [
    "# regularization hyperparameters\n",
    "penalties = [\"l1\", \"l2\"]\n",
    "solvers = ['liblinear', 'saga']\n",
    "\n",
    "# add labels back to the dataframe\n",
    "added_rows[0][\"LABEL\"] = added_rows[1]\n",
    "\n",
    "# gets the best score and parameters for logistic regression\n",
    "best_score, best_params = logistic_hypertune(added_rows, 5, penalties, solvers, [], 'LABEL', 1, 0, logistic_regression, [])\n",
    "\n",
    "print(best_score)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.573, 0.7862069, 0.945, 0.2017699, 0.3211267)\n"
     ]
    }
   ],
   "source": [
    "# remove labels again \n",
    "added_rows_cleaned = added_rows[0].drop(columns=\"LABEL\")\n",
    "\n",
    "# calculate logistic regression test predictions \n",
    "test_preds = logistic_regression(added_rows_cleaned, added_rows[1], added_rows_test[0], 0, 'l1', 'liblinear')\n",
    "\n",
    "# gets metrics\n",
    "best_metrics_logreg = model_metrics(added_rows_test[1].tolist(), test_preds)\n",
    "print(best_metrics_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN With LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shapes:  (5050, 3197) (5050,)\n"
     ]
    }
   ],
   "source": [
    "# add labels back to training/testing dataframes\n",
    "added_rows[0][\"LABEL\"] = added_rows[1]\n",
    "added_rows_test[0][\"LABEL\"] = added_rows_test[1]\n",
    "\n",
    "# shuffle the dataframes and sample random 50% of data for RNN training\n",
    "added_rows_shuffled = added_rows[0].sample(frac=0.5).reset_index(drop=True)\n",
    "added_rows_test_shuffled = added_rows_test[0].sample(frac=0.5).reset_index(drop=True)\n",
    "\n",
    "# split the training/testing dataframes to their respective features and labels\n",
    "added_rows_shuffled_features = split_x_and_outcome(added_rows_shuffled, \"LABEL\")[0].to_numpy()\n",
    "added_rows_shuffled_labels = split_x_and_outcome(added_rows_shuffled, \"LABEL\")[1].to_numpy(dtype=\"float64\")\n",
    "added_rows_shuffled_features_test = split_x_and_outcome(added_rows_test_shuffled, \"LABEL\")[0].to_numpy()\n",
    "added_rows_shuffled_labels_test = split_x_and_outcome(added_rows_test_shuffled, \"LABEL\")[1].to_numpy(dtype=\"float64\")\n",
    "\n",
    "# print shapes of the training data\n",
    "print(\"training shapes: \", added_rows_shuffled_features.shape, added_rows_shuffled_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features_for_rnn(features, num_blocks):\n",
    "    # get the length of each block of fluxes\n",
    "    block_len = features.shape[1] // num_blocks\n",
    "    # split up the features into blocks, truncate features to be divisible\n",
    "    # by block length\n",
    "    features_blocks = features[:, :num_blocks*block_len] \n",
    "    # reshape the array\n",
    "    features_blocks = features_blocks.reshape((features.shape[0], num_blocks, block_len))\n",
    "    return features_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_labels_for_rnn(labels, num_blocks):\n",
    "    # for each row, make an array of length num_blocks of the label of that row \n",
    "    # e.g (0, 0, 0, 0) or (1, 1, 1, 1)\n",
    "    labels_blocks = np.repeat(labels[:, np.newaxis], num_blocks, axis=1)\n",
    "    return labels_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the features for the RNN\n",
    "flux_for_training = preprocess_features_for_rnn(added_rows_shuffled_features, 5)\n",
    "flux_for_testing = preprocess_features_for_rnn(added_rows_shuffled_features_test, 5)\n",
    "\n",
    "# preprocess the labels for RNN\n",
    "flux_labels_for_training = preprocess_labels_for_rnn(added_rows_shuffled_labels, 5)\n",
    "flux_labels_for_testing = preprocess_labels_for_rnn(added_rows_shuffled_labels_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5050, 5, 639)\n",
      "(5050, 5)\n",
      "(565, 5, 639)\n",
      "(565, 5)\n"
     ]
    }
   ],
   "source": [
    "# printing the shapes of each of the training/testing features/labels\n",
    "print(flux_for_training.shape)\n",
    "print(flux_labels_for_training.shape)\n",
    "print(flux_for_testing.shape)\n",
    "print(flux_labels_for_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn(X, y, num_epochs=3, learning_rate=0.01, lstm_units=5, batch_size=64):\n",
    "    # initializing Sequential input layer\n",
    "    model = Sequential()\n",
    "    # LSTM layer with parameterized # of units\n",
    "    model.add(LSTM(lstm_units, input_shape=(X.shape[1]*X.shape[2], 1)))\n",
    "    # Dense output layer using sigmoid for activation (binary classification)\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # initialize stochastic gradient descent optimizer with parameterized learning rate\n",
    "    optimizer = SGD(learning_rate=learning_rate)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    # reshape features\n",
    "    X = X.reshape((X.shape[0], X.shape[1]*X.shape[2], 1))\n",
    "    # get y's as integers\n",
    "    y_binary = np.array([int(np.all(row)) for row in y])\n",
    "    # fit the model\n",
    "    history = model.fit(X, y_binary, epochs=num_epochs, batch_size=64)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fit of model on training data and return the accuracy for the epoch\n",
    "\n",
    "\n",
    "def calc_avg_val_accuracy_rnn(training_features, training_labels, num_epochs, learning_rate, lstm_units):\n",
    "    \n",
    "    best_vals = []\n",
    "    best_accuracy = 0\n",
    "    # for each epoch\n",
    "    for epoch in num_epochs:\n",
    "        # for each learning rate\n",
    "        for rate in learning_rate:\n",
    "            # for each number of LSTM units\n",
    "            for units in lstm_units:   \n",
    "                # run the RNN and get epoch data\n",
    "                history = init_rnn(training_features, training_features, epoch, rate, units)[1]\n",
    "                # calculate the average accuracy of the training iteration\n",
    "                avg_acc = np.mean(history.history['accuracy'])\n",
    "                # if the average accuracy greater than the best, keep track of parameters\n",
    "                if avg_acc > best_accuracy:\n",
    "                    best_accuracy = avg_acc\n",
    "                    best_vals = [epoch, rate, units]\n",
    "    \n",
    "    # return best hyperparmaeters and best accuracy for RNN training\n",
    "    return best_accuracy, best_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 3)                 60        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcf09fa6dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcf09fa6dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 51s 622ms/step - loss: 0.7221 - accuracy: 0.4723\n",
      "Epoch 2/3\n",
      "79/79 [==============================] - 50s 639ms/step - loss: 0.7063 - accuracy: 0.4877\n",
      "Epoch 3/3\n",
      "79/79 [==============================] - 48s 613ms/step - loss: 0.6944 - accuracy: 0.5240\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 7)                 252       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 260\n",
      "Trainable params: 260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcec3bcd7a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcec3bcd7a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 54s 661ms/step - loss: 0.7005 - accuracy: 0.4341\n",
      "Epoch 2/3\n",
      "79/79 [==============================] - 51s 647ms/step - loss: 0.6990 - accuracy: 0.4257\n",
      "Epoch 3/3\n",
      "79/79 [==============================] - 51s 645ms/step - loss: 0.6977 - accuracy: 0.4071\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 3)                 60        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcec3bcdd40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcec3bcdd40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 50s 618ms/step - loss: 0.6849 - accuracy: 0.5368\n",
      "Epoch 2/3\n",
      "79/79 [==============================] - 49s 624ms/step - loss: 0.6845 - accuracy: 0.5384\n",
      "Epoch 3/3\n",
      "79/79 [==============================] - 49s 618ms/step - loss: 0.6841 - accuracy: 0.5402\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 7)                 252       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 260\n",
      "Trainable params: 260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce6785d8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce6785d8c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 54s 662ms/step - loss: 0.6965 - accuracy: 0.5190\n",
      "Epoch 2/3\n",
      "79/79 [==============================] - 51s 650ms/step - loss: 0.6960 - accuracy: 0.5186\n",
      "Epoch 3/3\n",
      "79/79 [==============================] - 52s 654ms/step - loss: 0.6956 - accuracy: 0.5168\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 3)                 60        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce67ad0560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce67ad0560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 51s 619ms/step - loss: 0.7070 - accuracy: 0.5057\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 48s 607ms/step - loss: 0.6998 - accuracy: 0.5349\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 50s 631ms/step - loss: 0.6963 - accuracy: 0.5125\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 49s 618ms/step - loss: 0.6938 - accuracy: 0.5194\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 48s 612ms/step - loss: 0.6913 - accuracy: 0.5236\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_5 (LSTM)               (None, 7)                 252       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 260\n",
      "Trainable params: 260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce6d2f3d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce6d2f3d40> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 53s 651ms/step - loss: 0.6890 - accuracy: 0.5428\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 51s 647ms/step - loss: 0.6854 - accuracy: 0.5824\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 52s 656ms/step - loss: 0.6819 - accuracy: 0.6036\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 51s 652ms/step - loss: 0.6784 - accuracy: 0.6099\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 52s 657ms/step - loss: 0.6747 - accuracy: 0.6149\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 3)                 60        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcec3d79170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcec3d79170> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 49s 604ms/step - loss: 0.6955 - accuracy: 0.5067\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 49s 625ms/step - loss: 0.6954 - accuracy: 0.5069\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 49s 624ms/step - loss: 0.6953 - accuracy: 0.5077\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 47s 597ms/step - loss: 0.6952 - accuracy: 0.5071\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 48s 613ms/step - loss: 0.6950 - accuracy: 0.5063\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_7 (LSTM)               (None, 7)                 252       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 260\n",
      "Trainable params: 260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce676138c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce676138c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 53s 647ms/step - loss: 0.6956 - accuracy: 0.5446\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 52s 662ms/step - loss: 0.6954 - accuracy: 0.5537\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 51s 648ms/step - loss: 0.6952 - accuracy: 0.5604\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 51s 648ms/step - loss: 0.6950 - accuracy: 0.5648\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 52s 658ms/step - loss: 0.6948 - accuracy: 0.5679\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 3)                 60        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/7\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce67f66560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce67f66560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 50s 615ms/step - loss: 0.7131 - accuracy: 0.5113\n",
      "Epoch 2/7\n",
      "79/79 [==============================] - 50s 634ms/step - loss: 0.7075 - accuracy: 0.5285\n",
      "Epoch 3/7\n",
      "79/79 [==============================] - 48s 607ms/step - loss: 0.7030 - accuracy: 0.5317\n",
      "Epoch 4/7\n",
      "79/79 [==============================] - 49s 625ms/step - loss: 0.6984 - accuracy: 0.5311\n",
      "Epoch 5/7\n",
      "79/79 [==============================] - 51s 642ms/step - loss: 0.6930 - accuracy: 0.5370\n",
      "Epoch 6/7\n",
      "79/79 [==============================] - 49s 617ms/step - loss: 0.6861 - accuracy: 0.5442\n",
      "Epoch 7/7\n",
      "79/79 [==============================] - 50s 639ms/step - loss: 0.6775 - accuracy: 0.5545\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_9 (LSTM)               (None, 7)                 252       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 260\n",
      "Trainable params: 260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/7\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce6865ba70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce6865ba70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 53s 653ms/step - loss: 0.6963 - accuracy: 0.4505\n",
      "Epoch 2/7\n",
      "79/79 [==============================] - 52s 657ms/step - loss: 0.6938 - accuracy: 0.5224\n",
      "Epoch 3/7\n",
      "79/79 [==============================] - 52s 652ms/step - loss: 0.6915 - accuracy: 0.5325\n",
      "Epoch 4/7\n",
      "79/79 [==============================] - 52s 652ms/step - loss: 0.6893 - accuracy: 0.5483\n",
      "Epoch 5/7\n",
      "79/79 [==============================] - 51s 641ms/step - loss: 0.6873 - accuracy: 0.5620\n",
      "Epoch 6/7\n",
      "79/79 [==============================] - 52s 654ms/step - loss: 0.6853 - accuracy: 0.5762\n",
      "Epoch 7/7\n",
      "79/79 [==============================] - 51s 648ms/step - loss: 0.6832 - accuracy: 0.5812\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_10 (LSTM)              (None, 3)                 60        \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/7\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce6865bc20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce6865bc20> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 52s 631ms/step - loss: 0.6851 - accuracy: 0.5440\n",
      "Epoch 2/7\n",
      "79/79 [==============================] - 49s 620ms/step - loss: 0.6850 - accuracy: 0.5459\n",
      "Epoch 3/7\n",
      "79/79 [==============================] - 49s 617ms/step - loss: 0.6848 - accuracy: 0.5489\n",
      "Epoch 4/7\n",
      "79/79 [==============================] - 50s 631ms/step - loss: 0.6846 - accuracy: 0.5501\n",
      "Epoch 5/7\n",
      "79/79 [==============================] - 49s 621ms/step - loss: 0.6845 - accuracy: 0.5517\n",
      "Epoch 6/7\n",
      "79/79 [==============================] - 50s 638ms/step - loss: 0.6843 - accuracy: 0.5554\n",
      "Epoch 7/7\n",
      "79/79 [==============================] - 49s 620ms/step - loss: 0.6842 - accuracy: 0.5560\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_11 (LSTM)              (None, 7)                 252       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 260\n",
      "Trainable params: 260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/7\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce67613560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fce67613560> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "79/79 [==============================] - 53s 651ms/step - loss: 0.6904 - accuracy: 0.5073\n",
      "Epoch 2/7\n",
      "79/79 [==============================] - 53s 665ms/step - loss: 0.6903 - accuracy: 0.5109\n",
      "Epoch 3/7\n",
      "79/79 [==============================] - 51s 648ms/step - loss: 0.6901 - accuracy: 0.5125\n",
      "Epoch 4/7\n",
      "79/79 [==============================] - 51s 650ms/step - loss: 0.6899 - accuracy: 0.5149\n",
      "Epoch 5/7\n",
      "79/79 [==============================] - 50s 637ms/step - loss: 0.6898 - accuracy: 0.5184\n",
      "Epoch 6/7\n",
      "79/79 [==============================] - 51s 650ms/step - loss: 0.6896 - accuracy: 0.5202\n",
      "Epoch 7/7\n",
      "79/79 [==============================] - 51s 647ms/step - loss: 0.6895 - accuracy: 0.5208\n",
      "0.5906930804252625\n",
      "[5, 0.01, 7]\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters for RNN\n",
    "num_epochs = [3, 5, 7]\n",
    "learning_rates = [0.01, 0.001]\n",
    "lstm_cells = [3, 7]\n",
    "\n",
    "# get best hyperparameters for validation\n",
    "best_acc, best_params = calc_avg_val_accuracy_rnn(flux_for_training, flux_labels_for_training, num_epochs, learning_rates, lstm_cells)\n",
    "print(best_acc)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_12 (LSTM)              (None, 7)                 252       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 8         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 260\n",
      "Trainable params: 260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcedecd4830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fcedecd4830> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "9/9 [==============================] - 7s 631ms/step - loss: 0.7192 - accuracy: 0.3593\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 6s 647ms/step - loss: 0.7167 - accuracy: 0.3611\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 6s 642ms/step - loss: 0.7144 - accuracy: 0.3628\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 6s 640ms/step - loss: 0.7121 - accuracy: 0.3646\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 6s 642ms/step - loss: 0.7100 - accuracy: 0.3593\n"
     ]
    }
   ],
   "source": [
    "# Reshape the testing features to have shape (565, 3195, 1)\n",
    "flux_for_test_reshaped = np.reshape(flux_for_testing, (flux_for_testing.shape[0], flux_for_testing.shape[1]*flux_for_testing.shape[2], 1))\n",
    "flux_for_test_labels_reshaped = np.all(flux_labels_for_testing, axis=1).astype(int)[:, np.newaxis]  # Reshape to (565, 1)\n",
    "\n",
    "\n",
    "# Evaluate the model on the reshaped testing data\n",
    "model = init_rnn(flux_for_test_reshaped, flux_for_test_labels_reshaped, 5, 0.01, 7, 64)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcf09fa4440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fcf09fa4440> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "18/18 [==============================] - 3s 132ms/step\n",
      "(0.361, 0.366548, 0.362, 0.3601399, 0.3633157)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the reshaped testing data\n",
    "y_pred_reshaped = model.predict(flux_for_test_reshaped)\n",
    "\n",
    "# the model outputs predictions between 0-1. Convert them to 0's and 1's based on if\n",
    "# they are less than or greater than 0.5\n",
    "def make_preds_into_binary(preds):\n",
    "    y_preds = []\n",
    "    for pred in preds:\n",
    "        if pred < 0.5:\n",
    "            y_preds.append(0)\n",
    "        else:\n",
    "            y_preds.append(1)\n",
    "            \n",
    "    return y_preds\n",
    "    \n",
    "y_preds = make_preds_into_binary(y_pred_reshaped)\n",
    "\n",
    "# get the first element of each list in a list of lists\n",
    "def extract(lst):\n",
    "    return [int(item[0]) for item in lst]\n",
    "\n",
    "# calculate metrics on the test predictions from RNN\n",
    "rnn_metrics = model_metrics(extract(flux_labels_for_testing.tolist()), y_preds)\n",
    "print(rnn_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
